{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98002194-d974-4b6f-8c35-30bbfc2d1717",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12daf62a-df01-4c84-acc9-9df77a89afdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random_seed import seed_everything, RANDOM_SEED\n",
    "seed_everything(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a21e9e4a-67bd-4f77-87fc-2ecc6b454819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hpsv2\n",
    "import os\n",
    "import optuna\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import lpips\n",
    "from torchvision.transforms import ToTensor\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "from torchvision.models.optical_flow import Raft_Large_Weights, raft_large\n",
    "\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b719c6-1046-44a3-9924-010ea9a3e3ec",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "267bf36c-d587-460a-9ce9-f534dad1f2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('data.xlsx', index_col=0)\n",
    "\n",
    "frame_rate = 1\n",
    "video_frames_folder = \"videos_frames\"\n",
    "\n",
    "prompts = df['prompt'].values.tolist()\n",
    "\n",
    "video_paths = []\n",
    "for video_name in df['video_name'].values:\n",
    "    video_name_without_ext = ''.join(video_name.split('.')[:-1])\n",
    "    video_full_path = os.path.join(video_frames_folder, video_name_without_ext)\n",
    "    video_full_path_frame_rate = os.path.join(video_full_path, f'frame_rate_{frame_rate}')\n",
    "    \n",
    "    video_paths.append(video_full_path_frame_rate)\n",
    "    \n",
    "scores = np.array(df['score'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a86342f-aa78-4333-b0fa-637b36472068",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "to_tensor = ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a252ca18-6a21-4374-9fa2-a81b58e92abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0222056e-c8fd-4fb3-8b57-8ed5357bbbd9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Feature mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f940d98-7e26-4b14-807f-022e145bd223",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## CLIP (prompt and frame similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98e2247f-a578-4c60-9263-1875160602ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_clip_similarity(model, processor, prompt, frame):\n",
    "    inputs = processor(text=[prompt], images=frame, return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits_per_image = outputs.logits_per_image.squeeze().item()\n",
    "    return logits_per_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac06036a-e57a-44ff-9edd-b63d987798d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### clip-vit-base-patch32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9c80cd7-7ffa-4932-a1e7-0c18be20a6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'clip-vit-base-patch32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "908d9085-2c00-4535-9e4e-4b0560a317ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained(f\"openai/{model_name}\").to(device).eval()\n",
    "processor = CLIPProcessor.from_pretrained(f\"openai/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b74a661-1ec7-4a5f-bcf7-a2e6ac879fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dct[model_name] = {}\n",
    "\n",
    "for i, (prompt, video_path) in enumerate(zip(prompts, video_paths)):\n",
    "    video_frames = [Image.open(f\"{video_path}/frame_{i:04d}.jpg\") for i in range(len(os.listdir(video_path)))]\n",
    "    scores = []\n",
    "    \n",
    "    for frame in video_frames:\n",
    "        result = calculate_clip_similarity(model, processor, prompt, frame)\n",
    "        scores.append(result)\n",
    "        \n",
    "    dct[model_name][prompt] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42963ad6-8ac6-4d12-9fd7-470b7f0f6081",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea32f1e-c341-4245-9ae9-434a58871aed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### clip-vit-large-patch14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b125f042-4d58-4577-911c-6d7d107766de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'clip-vit-large-patch14'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283f7893-bfcd-48b1-a7a0-f0cb33154ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained(f\"openai/{model_name}\").to(device).eval()\n",
    "processor = CLIPProcessor.from_pretrained(f\"openai/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8d0961-d2fb-44e9-87f6-b75c3bbcedc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dct[model_name] = {}\n",
    "\n",
    "for i, (prompt, video_path) in enumerate(zip(prompts, video_paths)):\n",
    "    video_frames = [Image.open(f\"{video_path}/frame_{i:04d}.jpg\") for i in range(len(os.listdir(video_path)))]\n",
    "    scores = []\n",
    "    \n",
    "    for frame in video_frames:\n",
    "        result = calculate_clip_similarity(model, processor, prompt, frame)\n",
    "        scores.append(result)\n",
    "        \n",
    "    dct[model_name][prompt] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3aafb1ee-7204-4e10-b397-fc8ca5df9230",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d05ab5-048f-49b3-b499-d4f0e4229f96",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## SigLIP (prompt and frame similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "63abb675-cf56-419e-8466-8f8db30cc0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_siglip_similarity(model, processor, prompt, frame):\n",
    "    inputs = processor(text=[prompt], images=frame, padding=\"max_length\", return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits_per_image = outputs.logits_per_image.squeeze().item()\n",
    "    return logits_per_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b945326-b04d-4c55-9b9f-eecda1c04cd5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### siglip-so400m-patch14-384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "00ee5c23-316b-4e5a-9452-20f6335006fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'siglip-so400m-patch14-384'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a9cb944d-5998-434f-9b33-f6b49798861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(f\"google/{model_name}\").to(device).eval()\n",
    "processor = AutoProcessor.from_pretrained(f\"google/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b1a01e-dd3a-49d0-b854-8a8095395b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dct[model_name] = {}\n",
    "\n",
    "for i, (prompt, video_path) in enumerate(zip(prompts, video_paths)):\n",
    "    video_frames = [Image.open(f\"{video_path}/frame_{i:04d}.jpg\") for i in range(len(os.listdir(video_path)))]\n",
    "    scores = []\n",
    "    \n",
    "    for frame in video_frames:\n",
    "        result = calculate_siglip_similarity(model, processor, prompt, frame)\n",
    "        scores.append(result)\n",
    "        \n",
    "    dct[model_name][prompt] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "689921ad-9649-4669-b0f1-0c882c599786",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4cbdb4-7e79-481f-8c3e-3c24d81ba3f4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### siglip-large-patch16-384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bc1dd1b3-2ee6-4161-be42-81496ab2e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'siglip-large-patch16-384'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b4070d-5d42-430d-b182-a6cfb0d0c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(f\"google/{model_name}\").to(device).eval()\n",
    "processor = AutoProcessor.from_pretrained(f\"google/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63876f9-3133-461c-87d3-8e3cd8088bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dct[model_name] = {}\n",
    "\n",
    "for i, (prompt, video_path) in enumerate(zip(prompts, video_paths)):\n",
    "    video_frames = [Image.open(f\"{video_path}/frame_{i:04d}.jpg\") for i in range(len(os.listdir(video_path)))]\n",
    "    scores = []\n",
    "    \n",
    "    for frame in video_frames:\n",
    "        result = calculate_siglip_similarity(model, processor, prompt, frame)\n",
    "        scores.append(result)\n",
    "        \n",
    "    dct[model_name][prompt] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c25deea-e058-4836-a898-c9f13621365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00833feb-e73f-4ebb-96af-12dae775c9a5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Jina-CLIP (prompt and frame similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e616ee0c-3741-4580-af81-d722067210d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_jinaclip_similarity(model, prompt, frame):\n",
    "    with torch.no_grad():\n",
    "        text_embedding = model.encode_text(prompt, task='retrieval.query', truncate_dim=512)\n",
    "        image_embedding = model.encode_image(frame, truncate_dim=512)\n",
    "\n",
    "    score = text_embedding @ image_embedding.T\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fbd32f3-df72-40d0-8555-bf9c398e7d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'jina-clip-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d9a5df8d-87a1-459d-835b-9b0506607ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(f'jinaai/{model_name}', trust_remote_code=True).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee29211-2fce-421f-844a-202d7dbbc744",
   "metadata": {},
   "outputs": [],
   "source": [
    "dct[model_name] = {}\n",
    "\n",
    "for i, (prompt, video_path) in enumerate(zip(prompts, video_paths)):\n",
    "    video_frames = [Image.open(f\"{video_path}/frame_{i:04d}.jpg\") for i in range(len(os.listdir(video_path)))]\n",
    "    scores = []\n",
    "    \n",
    "    for frame in video_frames:\n",
    "        result = calculate_jinaclip_similarity(model, prompt, frame)\n",
    "        scores.append(result)\n",
    "        \n",
    "    dct[model_name][prompt] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9088f9ce-861c-4f47-a9e7-990066bcb4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac00194-38a4-4246-8316-a9f79cfbf070",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## HPS (prompt and frame similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689f77ba-a755-44ff-b048-3af96fea61ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dct['hps'] = {}\n",
    "\n",
    "for i, (prompt, video_path) in enumerate(zip(prompts, video_paths)):\n",
    "    video_frames = [Image.open(f\"{video_path}/frame_{i:04d}.jpg\") for i in range(len(os.listdir(video_path)))]\n",
    "    \n",
    "    result = np.array(hpsv2.score(video_frames, prompt, hps_version=\"v2.1\"))\n",
    "    \n",
    "    dct['hps'][prompt] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e68d5b-f3d0-4d7c-9fe9-3e57c8fa5359",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## LPIPS (frames comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f727370-809e-4dda-8030-a5acb77db520",
   "metadata": {},
   "outputs": [],
   "source": [
    "dct['lpips'] = {}\n",
    "lpips_model = lpips.LPIPS(net=\"alex\").to(device)\n",
    "\n",
    "for i, (prompt, video_path) in enumerate(zip(prompts, video_paths)):\n",
    "    print(i)\n",
    "    video_frames = [Image.open(f\"{video_path}/frame_{i:04d}.jpg\") for i in range(len(os.listdir(video_path)))]\n",
    "    tensor_frames = [to_tensor(video_frame.convert(\"RGB\")).unsqueeze(0).to(device) for video_frame in video_frames]\n",
    "    \n",
    "    lpips_distances = []\n",
    "    for j in range(len(tensor_frames) - 1):\n",
    "        frame1 = tensor_frames[j]\n",
    "        frame2 = tensor_frames[j + 1]\n",
    "        \n",
    "        distance = lpips_model(frame1, frame2).item()\n",
    "        lpips_distances.append(distance)\n",
    "    \n",
    "    dct['lpips'][prompt] = np.array(lpips_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988942aa-38fb-4fd5-901a-d4f334f190eb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## SSIM (frames comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dc2d4b-f9aa-4a92-8c62-4c3262b15767",
   "metadata": {},
   "outputs": [],
   "source": [
    "dct['ssim'] = {}\n",
    "\n",
    "for i, (prompt, video_path) in enumerate(zip(prompts, video_paths)):\n",
    "    video_frames = [Image.open(f\"{video_path}/frame_{i:04d}.jpg\").convert(\"L\") for i in range(len(os.listdir(video_path)))]\n",
    "    print(i)\n",
    "    ssim_scores = []\n",
    "    for j in range(len(video_frames) - 1):\n",
    "        frame1 = np.array(video_frames[j])\n",
    "        frame2 = np.array(video_frames[j + 1])\n",
    "        \n",
    "        score, _ = ssim(frame1, frame2, full=True)\n",
    "        ssim_scores.append(score)\n",
    "    \n",
    "    dct['ssim'][prompt] = np.array(ssim_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1edbb40-ca7e-4c8e-bf5e-5539c26d3dfe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Optical Flow (frames comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f9ba6fcd-4fd4-4f85-b668-8729e21d70d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dct['optical_flow'] = {}\n",
    "weights = Raft_Large_Weights.DEFAULT\n",
    "model = raft_large(weights=weights, progress=False).to(device)\n",
    "transform = weights.transforms()\n",
    "\n",
    "def get_features(x):\n",
    "    return torch.tensor((\n",
    "        x.sum(),\n",
    "        x.median(),\n",
    "        x.mean(),\n",
    "        x.std(),\n",
    "        x.min(),\n",
    "        x.max()\n",
    "    ))\n",
    "\n",
    "for i, (prompt, video_path) in enumerate(zip(prompts, video_paths)):\n",
    "    video_frames = [Image.open(f\"{video_path}/frame_{i:04d}.jpg\").convert(\"RGB\") for i in range(len(os.listdir(video_path)))]\n",
    "    \n",
    "    optical_flow_features = []\n",
    "    \n",
    "    for j in range(len(video_frames) - 1):\n",
    "        frame1_tensor = to_tensor(video_frames[j]).unsqueeze(0).to(device)\n",
    "        frame2_tensor = to_tensor(video_frames[j + 1]).unsqueeze(0).to(device)\n",
    "        \n",
    "        frame_1, frame_2 = transform(frame1_tensor, frame2_tensor)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            flow = model(frame_1, frame_2)[-1]\n",
    "        \n",
    "        flow = flow.squeeze(0)\n",
    "        flow_features = get_features(flow)\n",
    "        \n",
    "        magnitude = torch.sqrt(flow[0]**2 + flow[1]**2)\n",
    "        magnitude_features = get_features(magnitude)\n",
    "        \n",
    "        angle = torch.arctan2(flow[0], flow[1])\n",
    "        angle_features = get_features(angle)\n",
    "        \n",
    "        optical_flow_features.append(torch.cat([flow_features, magnitude_features, angle_features]))\n",
    "\n",
    "    dct['optical_flow'][prompt] = np.array(optical_flow_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bb472b-1432-49bf-838f-bf86692b08c5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9c283e-8f53-4add-8a54-d2ef916dd9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dict_with_features.pickle', 'wb') as f:\n",
    "    pickle.dump(dct, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ccc362a3-dce8-4733-b16f-40bc9ec54a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = []\n",
    "\n",
    "for key, value in dct['hps'].items():\n",
    "    matrix.append(len(value))\n",
    "\n",
    "matrix = np.array(matrix)\n",
    "\n",
    "train, test = train_test_split(\n",
    "    matrix,\n",
    "    test_size=0.2, \n",
    "    random_state=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e71242a6-43f1-4abe-8b19-1b738495ede5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121.0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(train) # среднее значение на train количества фреймов в видео"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4253f6aa-f153-4561-bfaa-a908ba9b5cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f28fcb8-05ef-45dc-b457-ac2ee96175e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ema(values, alpha):\n",
    "    ema_pred = values[0]\n",
    "    ema = None\n",
    "    \n",
    "    for value in values:\n",
    "        ema = alpha * value + (1 - alpha) * ema_pred\n",
    "        ema_pred = ema\n",
    "\n",
    "    return ema\n",
    "\n",
    "def get_features(values):\n",
    "    ema_features = [\n",
    "        calculate_ema(values, alpha=0.1),\n",
    "        calculate_ema(values, alpha=0.3),\n",
    "        calculate_ema(values, alpha=0.5),\n",
    "        calculate_ema(values, alpha=0.7),\n",
    "        calculate_ema(values, alpha=0.9)\n",
    "    ]\n",
    "    \n",
    "    features = [\n",
    "        np.sum(values), \n",
    "        np.mean(values), \n",
    "        np.median(values),\n",
    "        np.std(values),\n",
    "        np.min(values),\n",
    "        np.max(values),\n",
    "        np.quantile(values, 0.25),\n",
    "        np.quantile(values, 0.75),\n",
    "        np.quantile(values, 0.75) - np.quantile(values, 0.25),\n",
    "        np.min(values) / np.max(values)\n",
    "    ]\n",
    "    \n",
    "    return ema_features + features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "eac30229-ae98-45b3-99b9-eb3d97a02bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dct = {}\n",
    "\n",
    "for prompt in prompts:\n",
    "    \n",
    "    video_features = []\n",
    "    \n",
    "    for key in sorted(dct.keys()):\n",
    "        if key != 'optical_flow':\n",
    "            values = dct[key][prompt]           \n",
    "            \n",
    "            new_features = get_features(values) # новые фичи по значениям ряда\n",
    "            \n",
    "            frames_values = values[:num_frames]\n",
    "            \n",
    "            if len(frames_values) < num_frames:\n",
    "                tmp = np.zeros(num_frames)\n",
    "                tmp[:len(frames_values)] = frames_values\n",
    "                frames_values = tmp\n",
    "            \n",
    "            all_features = np.concatenate((new_features, frames_values))\n",
    "        else:\n",
    "            values = dct[key][prompt]\n",
    "            \n",
    "            new_features_lst = [get_features(values[:, i]) for i in range(values.shape[1])]\n",
    "            all_features = [feature for features_lst in new_features_lst for feature in features_lst]\n",
    "            \n",
    "        video_features += list(all_features)\n",
    "\n",
    "    feature_dct[prompt] = video_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e7a9b8ba-e877-4454-81ae-ff7358e5fe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = len(feature_dct[prompt])\n",
    "num_objs = len(df)\n",
    "\n",
    "matrix = np.zeros((num_objs, num_features + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f4223181-ea22-417b-b294-4f646db17489",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (prompt, score) in enumerate(zip(prompts, scores)):\n",
    "    features = np.array(feature_dct[prompt])\n",
    "    \n",
    "    matrix[i, :len(features)] = features\n",
    "    matrix[i, matrix.shape[1] - 1] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "eb28b71e-452b-47e4-b737-d60c5cbe5777",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_lst = []\n",
    "\n",
    "aggregations = [\n",
    "        'ema_01',\n",
    "        'ema_03',\n",
    "        'ema_05',\n",
    "        'ema_07',\n",
    "        'ema_09',\n",
    "        'sum',\n",
    "        'mean',\n",
    "        'median',\n",
    "        'std',\n",
    "        'min',\n",
    "        'max',\n",
    "        'q_25',\n",
    "        'q_75',\n",
    "        'IQR',\n",
    "        'min/max'\n",
    "    ]\n",
    "\n",
    "optical_flow_aggregations = [\n",
    "    'sum',\n",
    "    'median',\n",
    "    'mean',\n",
    "    'std',\n",
    "    'min',\n",
    "    'max',\n",
    "]\n",
    "\n",
    "for key in sorted(dct.keys()):\n",
    "    if key == 'optical_flow':\n",
    "        for upper_aggregation in aggregations:\n",
    "            for type_metric in ['flow', 'magnitude', 'angel']:\n",
    "                for aggregation in optical_flow_aggregations:\n",
    "                    feature_lst.append(f'{key}_{upper_aggregation}_{type_metric}_{aggregation}')\n",
    "    else:\n",
    "        for aggregation in aggregations:\n",
    "            feature_lst.append(f'{key}_{aggregation}')\n",
    "    \n",
    "        for num_frame in range(num_frames):\n",
    "            feature_lst.append(f'{key}_{num_frame}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b9801d4f-144a-49a0-accd-9ace11fae278",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = pd.DataFrame(data=matrix, columns=feature_lst + ['target'])\n",
    "feature_df['prompt'] = prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "95b8cb78-c06d-4646-a7f2-a89e41468050",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.to_csv('feature_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
